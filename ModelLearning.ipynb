{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from langdetect import detect\n",
    "import cPickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return cPickle.load(f)\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/' + name + '.pkl', 'wb') as f:\n",
    "        cPickle.dump(obj, f)#,  marshal.version)#, marshal.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "class FeatureGenerator:\n",
    "    \n",
    "    def __init__(self, lm):\n",
    "        self.lm = lm\n",
    "    \n",
    "    def generate_features(self, query):\n",
    "        x = []\n",
    "        words = re.findall(ur\"(?u)\\w+\", query)\n",
    "        x.append(len(words))# количество слов\n",
    "        x.append(len(query))# количество символов\n",
    "        x.append(self.lm.get_prob(words)) # вероятность такого запроса\n",
    "        max_prob = -1.\n",
    "        min_prob = 2.\n",
    "        \n",
    "        count_of_words_in_dict = 0\n",
    "        for word in words:\n",
    "            prob = self.lm.get_word_prob(word)\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "            if prob < min_prob:\n",
    "                min_prob = prob\n",
    "                \n",
    "            if self.lm.dict.has_key(word):\n",
    "                count_of_words_in_dict += 1\n",
    "                \n",
    "        x.append(max_prob) # максимальная вероятность слова\n",
    "        x.append(min_prob) # минимальная вероятность слова\n",
    "        x.append(len(words)-count_of_words_in_dict) # сколько слов нет в словаре\n",
    "        \n",
    "        if u\",\" in query or \\\n",
    "            u\".\" in query or \\\n",
    "            u\"'\" in query or \\\n",
    "            u\";\" in query or \\\n",
    "            u\"]\" in query or \\\n",
    "            u\"[\" in query or \\\n",
    "            u\"~\" in query:\n",
    "            x.append(1) # есть ли \"плохие\" символы в запросе \n",
    "        else:\n",
    "            x.append(0)\n",
    "        \n",
    "        try:\n",
    "            lang = detect(query)\n",
    "            lang = 1 if lang == 'en' else 0\n",
    "        except Exception:\n",
    "            lang = 0\n",
    "            \n",
    "        x.append(lang) # язык\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = load_obj(\"LanguageModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "fg = FeatureGenerator(lm)\n",
    "\n",
    "with open(\"data/middle.txt\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "for line in content:\n",
    "\n",
    "    line = line.decode(\"utf-8\")\n",
    "    line = line.lower()\n",
    "    line = line[:-1]\n",
    "    queries = line.split('\\t')\n",
    "\n",
    "    if len(queries) == 2:\n",
    "        y.append(0)\n",
    "        X.append(fg.generate_features(queries[0]))\n",
    "        \n",
    "        y.append(1)\n",
    "        X.append(fg.generate_features(queries[1]))\n",
    "        \n",
    "    else:\n",
    "        y.append(1)\n",
    "        X.append(fg.generate_features(queries[0]))\n",
    "    #if index > 0:\n",
    "    #line = line[index+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830981522403\n",
      "0.988017638037\n"
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "acc = []\n",
    "kf = KFold(len(y), n_folds=4, shuffle=True)\n",
    "for train_index, test_index in kf:\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    \n",
    "    gb = GradientBoostingClassifier(n_estimators=200, loss='exponential')\n",
    "    gb.fit(X_train, y_train)\n",
    "    \n",
    "    y_predicted = gb.predict(X_test)\n",
    "    f1.append(f1_score(y_test, y_predicted, pos_label=0))\n",
    "    acc.append(accuracy_score(y_test, y_predicted))\n",
    "    \n",
    "print sum(f1)/len(f1)\n",
    "print sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='exponential',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=200, loss='exponential')\n",
    "gb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_obj(gb, \"classifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
